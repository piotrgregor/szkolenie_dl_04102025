{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f288ccd43487d87e",
   "metadata": {},
   "source": [
    "# TensorFlow – czym jest i do czego służy?\n",
    "\n",
    "**TensorFlow** to jedna z najpopularniejszych bibliotek do uczenia maszynowego i głębokiego uczenia, stworzona i rozwijana przez **Google Brain Team** (premiera w 2015 roku). Jest napisana w **C++**, z interfejsem w **Pythonie**, a także z wersjami dla języków takich jak JavaScript czy Swift.\n",
    "\n",
    "### Kluczowe cechy\n",
    "\n",
    "* **Statyczne grafy obliczeń (w oryginalnej wersji)** – model był definiowany jako cały graf, a następnie wykonywany. To dawało dużą wydajność i łatwość optymalizacji, ale początkowo utrudniało debugowanie.\n",
    "* **Keras jako wysoki poziom** – od TensorFlow 2.0 domyślnie integruje API **Keras**, co bardzo ułatwia budowanie i trenowanie modeli.\n",
    "* **Wydajność i skalowalność** – TensorFlow jest zoptymalizowany pod kątem dużych modeli i rozproszonych obliczeń (np. na wielu GPU lub w chmurze).\n",
    "* **Wsparcie dla produkcji** – ma narzędzia takie jak **TensorFlow Serving**, **TensorFlow Lite** (na urządzenia mobilne) i **TensorFlow.js** (w przeglądarce), które ułatwiają wdrażanie modeli.\n",
    "* **Eager execution** – od TensorFlow 2.0 działa w trybie „na żywo” (dynamicznym), podobnie jak PyTorch, ale wciąż pozwala przełączać się na grafy statyczne dla optymalizacji.\n",
    "\n",
    "### Do czego jest używany?\n",
    "\n",
    "* **Deep learning** – trenowanie modeli CNN, RNN, transformerów.\n",
    "* **Wizja komputerowa** – rozpoznawanie obrazów, detekcja obiektów (często z biblioteką **tf.keras.applications**).\n",
    "* **Przetwarzanie języka naturalnego (NLP)** – od klasyfikacji tekstów po nowoczesne modele językowe.\n",
    "* **Duże wdrożenia produkcyjne** – dzięki wsparciu Google Cloud i narzędziom do serwowania modeli.\n",
    "* **IoT i mobile** – TensorFlow Lite umożliwia uruchamianie modeli na smartfonach, Raspberry Pi czy mikrokontrolerach.\n",
    "\n",
    "### Dlaczego jest popularny?\n",
    "\n",
    "TensorFlow był pierwszym dużym, otwartym frameworkiem wspieranym przez gigantyczną firmę (Google), dlatego szybko stał się standardem przemysłowym. Dzięki ogromnemu ekosystemowi i integracji z Kerasem jest szeroko używany zarówno w badaniach, jak i w zastosowaniach komercyjnych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7263e0eeaa28a03",
   "metadata": {},
   "source": [
    "## TensorFlow 1.x vs TensorFlow 2.x\n",
    "\n",
    "### TensorFlow 1.x (2015–2019)\n",
    "\n",
    "* **Statyczny graf obliczeń**: najpierw budujesz cały graf (np. `tf.placeholder`, `tf.Session`), potem go uruchamiasz.\n",
    "* **Kod mniej „pythoniczny”** – wymagał znajomości specyficznych konstrukcji (`tf.cond`, `tf.while_loop`) zamiast zwykłych `if` i `for`.\n",
    "* **Trudniejsze debugowanie** – bo graf był wykonywany dopiero w sesji, nie „na żywo”.\n",
    "* **Duża moc i optymalizacja** – świetny do produkcji i rozproszonych obliczeń, ale trudniejszy dla początkujących.\n",
    "\n",
    "### TensorFlow 2.x (od 2019)\n",
    "\n",
    "* **Domyślnie „eager execution”** – działa podobnie jak PyTorch: obliczenia wykonywane są natychmiast.\n",
    "* **Keras w centrum** – `tf.keras` stał się oficjalnym, wysokopoziomowym API do budowania i trenowania modeli.\n",
    "* **Lepsza integracja z Pythonem** – można pisać kod z `if`, `for`, itp. (dynamiczny graf).\n",
    "* **Zachowane tryby statyczne** – dzięki `@tf.function` można nadal „zamrozić” graf i uzyskać optymalizację w produkcji.\n",
    "* **Łatwiejsza nauka i migracja** – kod jest krótszy, czytelniejszy, bardziej intuicyjny.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122542ffc077cd8",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Keras to tak naprawdę **wysokopoziomowe API** (interfejs) do budowania i trenowania sieci neuronowych w Pythonie.\n",
    "\n",
    "🔹 **Krótko historycznie:**\n",
    "\n",
    "* Powstał w **2015 roku**, autor: **François Chollet** (inżynier Google, twórca też frameworka `Xception`).\n",
    "* Początkowo działał jako „nakładka” na różne backendy (np. TensorFlow, Theano, CNTK).\n",
    "* Od **TensorFlow 2.0** Keras został wbudowany w TensorFlow jako **tf.keras** i stał się jego oficjalnym API.\n",
    "\n",
    "---\n",
    "\n",
    "### Po co jest Keras?\n",
    "\n",
    "* Upraszcza tworzenie modeli: zamiast pisać dziesiątki linijek kodu, można w kilku krokach zbudować i wytrenować sieć neuronową.\n",
    "* Ukrywa szczegóły techniczne (np. graf obliczeń, zarządzanie sesjami, optymalizację GPU).\n",
    "* Jest intuicyjny – modele buduje się prawie jak z klocków Lego.\n",
    "\n",
    "---\n",
    "\n",
    "### Główne cechy Keras:\n",
    "\n",
    "1. **Prostota** – łatwy start dla początkujących.\n",
    "2. **Czytelność** – kod przypomina zwykły Python, a nie niszowy DSL.\n",
    "3. **Elastyczność** – mimo prostoty, można też tworzyć bardzo złożone modele.\n",
    "4. **Integracja z TensorFlow** – korzysta z jego mocy obliczeniowej (GPU/TPU, rozproszone trenowanie).\n",
    "\n",
    "---\n",
    "\n",
    "### Przykład – sieć klasyfikująca cyfry (MNIST)\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Model sekwencyjny\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),    # wejście 28x28\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "✅ W kilkunastu linijkach mamy kompletny proces trenowania.\n",
    "Gdyby pisać to od zera w „czystym” TensorFlow albo PyTorch – zajęłoby znacznie więcej kodu.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dane\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# --- Definicja parametrów (ręcznie) ---\n",
    "W1 = tf.Variable(tf.random.normal([784, 128]))\n",
    "b1 = tf.Variable(tf.zeros([128]))\n",
    "W2 = tf.Variable(tf.random.normal([128, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# --- Funkcja forward ---\n",
    "def model(x):\n",
    "    x = tf.reshape(x, [-1, 784])                  # flatten\n",
    "    h = tf.nn.relu(tf.matmul(x, W1) + b1)         # warstwa ukryta\n",
    "    y = tf.nn.softmax(tf.matmul(h, W2) + b2)      # wyjście\n",
    "    return y\n",
    "\n",
    "# --- Funkcja straty ---\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# --- Optymalizator ---\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# --- Pętla treningowa ---\n",
    "for epoch in range(5):\n",
    "    for i in range(len(x_train)):\n",
    "        x = x_train[i:i+1]\n",
    "        y_true = y_train[i:i+1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2]))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "👉 W skrócie: **Keras = przyjazna, prosta nakładka na TensorFlow**, która sprawia, że praca z sieciami neuronowymi jest szybsza i łatwiejsza.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45afd7e3aebf7fd",
   "metadata": {},
   "source": [
    "/## 1. TensorFlow vs PyTorch\n",
    "\n",
    "| Cecha                               | **TensorFlow**                                                                                       | **PyTorch**                                                            |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Rok powstania**                   | 2015 (Google Brain)                                                                                  | 2016 (Facebook AI Research)                                            |\n",
    "| **Graf obliczeń**                   | TF1: statyczny graf; TF2: domyślnie dynamiczny (eager), z opcją kompilacji do grafu (`@tf.function`) | Zawsze dynamiczny graf (budowany w trakcie wykonywania kodu)           |\n",
    "| **Produkcja / wdrażanie**           | Bardzo mocny ekosystem: TensorFlow Serving, TF Lite, TF.js                                           | TorchServe, TorchScript, ONNX – dobre, ale mniej dopracowane niż TF    |\n",
    "| **Społeczność**                     | Duże wsparcie przemysłowe (Google, firmy używające TF w chmurze)                                     | Popularniejszy w środowisku akademickim i badawczym                    |\n",
    "| **Uczenie rozproszone / wydajność** | Bardzo rozwinięte, integracja z TPU                                                                  | Bardzo dobre wsparcie GPU, integracja z CUDA, ostatnio też z Apple MPS |\n",
    "| **Krzywa nauki**                    | TF1 – trudna, TF2/Keras – łatwa                                                                      | Bardzo intuicyjna, świetna do nauki i eksperymentów                    |\n",
    "\n",
    "👉 Podsumowanie:\n",
    "\n",
    "* **TensorFlow** – świetny do **produkcji, dużych wdrożeń i skalowalności**.\n",
    "* **PyTorch** – idealny do **badań, eksperymentów i prototypowania**.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Keras vs PyTorch\n",
    "\n",
    "| Cecha              | **Keras (tf.keras)**                                                                      | **PyTorch**                                                                          |\n",
    "| ------------------ | ----------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| **Filozofia**      | Prosta, wysoki poziom, „modele z klocków Lego”                                            | Bardziej niskopoziomowy, ale elastyczny                                              |\n",
    "| **Łatwość startu** | Bardzo łatwy – parę linijek i masz sieć                                                   | Prosty, ale wymaga jawnego pisania pętli treningowej                                 |\n",
    "| **Elastyczność**   | Ograniczona – do typowych modeli (ale można pisać własne warstwy, trudniej niż w PyTorch) | Bardzo elastyczny – łatwo tworzyć nietypowe architektury, dynamiczne zmiany w modelu |\n",
    "| **Debugowanie**    | Działa w TF eager mode, ale bywa „ukryty” poziom                                          | Bardzo przejrzyste debugowanie (graf tworzony w trakcie)                             |\n",
    "| **Ekosystem**      | Wbudowane narzędzia TensorFlow (Lite, Serving, TPU)                                       | TorchVision, TorchText, TorchAudio, ONNX – mocne, ale mniej zunifikowane             |\n",
    "| **Dla kogo**       | Początkujący, szybkie prototypowanie, projekty produkcyjne Google Cloud                   | Badacze, osoby chcące pełnej kontroli i eksperymentowania                            |\n",
    "\n",
    "👉 Podsumowanie:\n",
    "\n",
    "* **Keras** = prostota, szybkie budowanie klasycznych modeli.\n",
    "* **PyTorch** = pełna kontrola i elastyczność, nawet kosztem większej ilości kodu.\n",
    "* **TensorFlow**  = duża wydajność, świetny na produkcję\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7f4087e392f39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cfa90195e5a9287",
   "metadata": {},
   "source": [
    "# Podstawy TensorFlow – Tensory\n",
    "\n",
    "W tej sekcji omówimy:\n",
    "\n",
    "* Konwersję tablic NumPy na tensory TensorFlow\n",
    "* Tworzenie tensorów od zera* Różnice względem podejścia w PyTorch\n",
    "\n",
    "## Wykonaj standardowe importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92512c08ef13f5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:12:28.631665Z",
     "start_time": "2025-09-30T17:11:50.587212Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603d25dab29d1c4",
   "metadata": {},
   "source": [
    "## Sprawdź wersję TensorFlow\n",
    "\n",
    "TensorFlow 2 domyślnie uruchamia obliczenia w trybie eager, więc operacje działają podobnie jak w PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37d9b0620f9a359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:12:28.679054Z",
     "start_time": "2025-09-30T17:12:28.675126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cdd3efbe21f65",
   "metadata": {},
   "source": [
    "## Konwersja tablic NumPy na tensory TensorFlow\n",
    "\n",
    "Analogicznie do `torch.tensor`, w TensorFlow używamy m.in. `tf.convert_to_tensor` lub `tf.constant`.\n",
    "\n",
    "Zauważ, że TensorFlow domyślnie tworzy tensory typu `tf.int64` dla liczb całkowitych i `tf.float32` dla wartości zmiennoprzecinkowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ce87287ed8502c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:13:16.933139Z",
     "start_time": "2025-09-30T17:13:16.898148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5], shape=(6,), dtype=int64)\n",
      "dtype: <dtype: 'int64'>\n",
      "Jako NumPy: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(6)\n",
    "x = tf.convert_to_tensor(arr)\n",
    "print(x)\n",
    "print('dtype:', x.dtype)\n",
    "print('Jako NumPy:', x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f584fd7649d64b",
   "metadata": {},
   "source": [
    "W przeciwieństwie do PyTorch, tensory TensorFlow są niemodyfikowalne (**immutable**).\n",
    "\n",
    "Zmiana tablicy źródłowej NumPy nie wpływa na tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e36ab32b97da7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:13:51.853157Z",
     "start_time": "2025-09-30T17:13:51.840759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zmodyfikowane arr: [999   1   2   3   4   5]\n",
      "Tensor x pozostał bez zmian: [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "arr[0] = 999\n",
    "\n",
    "print('Zmodyfikowane arr:', arr)\n",
    "\n",
    "print('Tensor x pozostał bez zmian:', x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f48c2789b44e7",
   "metadata": {},
   "source": [
    "## TensorFlow Variables\n",
    "\n",
    "Jeśli potrzebujemy obiektu, który można modyfikować \"w miejscu\", używamy `tf.Variable`.\n",
    "\n",
    "To różnica względem PyTorch, gdzie każdy tensor może przechowywać gradienty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1ceb6941ca2f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:14:25.045380Z",
     "start_time": "2025-09-30T17:14:25.012227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n",
      "Po assign_add: <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1.5, 2.5, 3.5], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable([1., 2., 3.], dtype=tf.float32)\n",
    "print(var)\n",
    "var.assign_add([0.5, 0.5, 0.5])\n",
    "print('Po assign_add:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8390b6c4d3d8f5a",
   "metadata": {},
   "source": [
    "## Tworzenie tensorów od zera\n",
    "\n",
    "Odpowiedniki funkcji `torch.zeros` czy `torch.ones` znajdują się w module `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40eb53e46eec9eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:14:57.909737Z",
     "start_time": "2025-09-30T17:14:57.892003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "ones: [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "fill: [[7 7 7]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "zeros = tf.zeros((2, 3))\n",
    "ones = tf.ones((2, 3), dtype=tf.float32)\n",
    "full = tf.fill((2, 3), 7)\n",
    "print('zeros:', zeros.numpy())\n",
    "print('ones:', ones.numpy())\n",
    "print('fill:', full.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346aefc1523cf8c9",
   "metadata": {},
   "source": [
    "## Losowe liczby\n",
    "\n",
    "TensorFlow udostępnia metody `tf.random.normal`, `tf.random.uniform` i wiele innych.\n",
    "\n",
    "Nazwy są nieco inne niż w PyTorch, ale działanie jest analogiczne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538e664a271954bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:15:24.578521Z",
     "start_time": "2025-09-30T17:15:24.553357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalny rozkład: [[-0.5798222   1.7611403  -0.09943233]\n",
      " [ 1.3010058   0.28214836  0.8640877 ]]\n",
      "Jednostajny rozkład: [[ 0.06529355  0.01903343  0.3020897 ]\n",
      " [-0.572804   -0.9006293   0.7291479 ]]\n"
     ]
    }
   ],
   "source": [
    "normal = tf.random.normal((2, 3), mean=0.0, stddev=1.0)\n",
    "uniform = tf.random.uniform((2, 3), minval=-1, maxval=1)\n",
    "print('Normalny rozkład:', normal.numpy())\n",
    "print('Jednostajny rozkład:', uniform.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a1d71225019d45",
   "metadata": {},
   "source": [
    "## Konwersja typów danych\n",
    "\n",
    "Do zmiany typu używamy `tf.cast`. To odpowiednik `tensor.type()` lub `tensor.to()` w PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44edbb926d7d1885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:15:42.990540Z",
     "start_time": "2025-09-30T17:15:42.980852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_float = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "x_float = tf.cast(x_float, tf.float32)\n",
    "print(x_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b8d1e97ba67aa",
   "metadata": {},
   "source": [
    "### Świetna robota!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
