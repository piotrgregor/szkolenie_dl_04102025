{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA - co to jest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA to platforma oblicze≈Ñ r√≥wnoleg≈Çych i model programowania, kt√≥re sprawiajƒÖ, ≈ºe u≈ºywanie GPU do oblicze≈Ñ og√≥lnego przeznaczenia staje siƒô proste i eleganckie. \n",
    "\n",
    "Programista pisze w znanym sobie jƒôzyku, takim jak C, C++, Python czy coraz szersza lista wspieranych jƒôzyk√≥w, a jedynie wykorzystuje rozszerzenia tych jƒôzyk√≥w w postaci kilku podstawowych s≈Ç√≥w kluczowych.\n",
    "\n",
    "Dziƒôki tym s≈Çowom kluczowym programista mo≈ºe wyraziƒá ogromny poziom r√≥wnoleg≈Ço≈õci i wskazaƒá kompilatorowi fragment aplikacji, kt√≥ry ma zostaƒá odwzorowany na GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiedy mo≈ºna u≈ºywaƒá PyTorch na GPU?\n",
    "\n",
    "* Potrzebujesz **karty graficznej NVIDIA** z obs≈ÇugƒÖ technologii **CUDA** (najlepiej nowsze modele).\n",
    "* Musisz mieƒá zainstalowane **sterowniki NVIDIA** (wspierajƒÖce wersjƒô CUDA zgodnƒÖ z PyTorch).\n",
    "* Je≈ºeli masz laptopa/PC tylko z kartƒÖ Intel/AMD ‚Äì wtedy GPU nie bƒôdzie obs≈Çugiwane i musisz korzystaƒá z wersji CPU.\n",
    "\n",
    "\n",
    "## Instalacja krok po kroku\n",
    "\n",
    "### Sprawd≈∫ GPU\n",
    "\n",
    "W terminalu / PowerShell:\n",
    "\n",
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "Je≈õli wy≈õwietli siƒô informacja o karcie i sterowniku ‚Äì mo≈ºesz i≈õƒá dalej.\n",
    "\n",
    "### Zainstaluj PyTorch z CUDA\n",
    "\n",
    "Na stronie [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) wybierz odpowiednie ustawienia (system, mened≈ºer pakiet√≥w, CUDA).\n",
    "\n",
    "Przyk≈Çadowo dla **pip + CUDA 12.6**:\n",
    "\n",
    "```bash\n",
    "pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "```\n",
    "\n",
    "### Sprawd≈∫ instalacjƒô\n",
    "\n",
    "W Pythonie:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```\n",
    "\n",
    "Je≈õli `torch.cuda.is_available()` zwraca `True`, to PyTorch widzi Twoje GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### Podsumowanie\n",
    "\n",
    "* **Mo≈ºesz u≈ºywaƒá PyTorch na GPU**, je≈õli masz kartƒô NVIDIA i zainstalowane sterowniki CUDA.\n",
    "* **Instalacja** r√≥≈ºni siƒô w zale≈ºno≈õci od systemu i wersji CUDA ‚Äì zawsze najlepiej korzystaƒá z konfiguratora na stronie PyTorch.\n",
    "* Je≈õli nie masz GPU od NVIDIA ‚Äì instalujesz zwyk≈ÇƒÖ wersjƒô CPU (`pip install torch torchvision torchaudio`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak u≈ºywaƒá CUDA zamiast CPU\n",
    "\n",
    "### Sprawdzenie dostƒôpno≈õci GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())   # True je≈õli CUDA dzia≈Ça\n",
    "# print(torch.cuda.get_device_name(0))  # nazwa karty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyb√≥r urzƒÖdzenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przeniesienie tensora / modelu na GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U≈ºywane urzƒÖdzenie: cpu\n",
      "Output: tensor([[-0.4420,  0.5694],\n",
      "        [-0.3411,  0.5518],\n",
      "        [-0.0780,  0.6204]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# wyb√≥r urzƒÖdzenia\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"U≈ºywane urzƒÖdzenie:\", device)\n",
    "\n",
    "# przyk≈Çadowy model: prosta warstwa liniowa\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 2)  # wej≈õcie: 3 cechy, wyj≈õcie: 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# dane wej≈õciowe\n",
    "x = torch.rand(3, 3).to(device)  # batch 3 pr√≥bek, ka≈ºda po 3 cechy\n",
    "\n",
    "# model na GPU/CPU\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# forward pass\n",
    "output = model(x)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üîπ Wa≈ºne zasady\n",
    "\n",
    "* **Model i dane muszƒÖ byƒá na tym samym urzƒÖdzeniu** (CPU albo GPU). W przeciwnym razie dostaniesz b≈ÇƒÖd typu `Expected all tensors to be on the same device`.\n",
    "* Mo≈ºesz korzystaƒá z wielu GPU przez `DataParallel` albo `DistributedDataParallel`.\n",
    "* Je≈õli chcesz wr√≥ciƒá na CPU, u≈ºywasz `.to(\"cpu\")`.\n",
    "üëâ Czy chcesz, ≈ºebym przygotowa≈Ç Ci **mini por√≥wnanie wydajno≈õci CPU vs GPU w PyTorch** na prostym przyk≈Çadzie (np. mno≈ºenie macierzy)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Google Colab\n",
    "\n",
    "GPU mo≈ºemy spr√≥bowac na Google Colab: https://colab.research.google.com/\n",
    "\n",
    "1. Otw√≥rz notebook w Google Colab.\n",
    "2. W menu wybierz: **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**.\n",
    "3. Kliknij **Save**.\n",
    "\n",
    "---\n",
    "\n",
    "### Sprawdzenie dostƒôpno≈õci GPU\n",
    "\n",
    "W pierwszej kom√≥rce uruchom:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "print(\"CUDA dostƒôpne:\", torch.cuda.is_available())\n",
    "print(\"Nazwa GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Brak\")\n",
    "```\n",
    "\n",
    "Je≈õli masz aktywne GPU, powinno zwr√≥ciƒá np. `\"Tesla T4\"` albo `\"A100\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Przyk≈Çad w Colab\n",
    "\n",
    "Mo≈ºesz od razu uruchomiƒá taki kod:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# wyb√≥r urzƒÖdzenia\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"U≈ºywane urzƒÖdzenie:\", device)\n",
    "\n",
    "# prosty model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# dane i model na GPU/CPU\n",
    "x = torch.rand(3, 3).to(device)\n",
    "model = MyModel().to(device)\n",
    "\n",
    "# forward\n",
    "output = model(x)\n",
    "print(\"Output:\", output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üí° W Colab GPU jest **wirtualne i wsp√≥≈Çdzielone**, wiƒôc wydajno≈õƒá mo≈ºe byƒá mniejsza ni≈º na Twojej w≈Çasnej karcie ‚Äì ale do nauki i test√≥w nadaje siƒô idealnie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## TPU\n",
    "\n",
    "* **TPU (Tensor Processing Unit)** to specjalny uk≈Çad scalony stworzony przez Google.\n",
    "* Jest zoptymalizowany do oblicze≈Ñ zwiƒÖzanych ze **sztucznƒÖ inteligencjƒÖ i uczeniem maszynowym** ‚Äì szczeg√≥lnie do **TensorFlow**.\n",
    "* To nie jest klasyczna karta graficzna (jak NVIDIA GPU), ale akcelerator zaprojektowany pod macierze i tensory.\n",
    "\n",
    "### üîπ CPU vs GPU vs TPU\n",
    "\n",
    "| Cecha                   | **CPU** (Central Processing Unit)                              | **GPU** (Graphics Processing Unit)                               | **TPU** (Tensor Processing Unit)                               |\n",
    "| ----------------------- | -------------------------------------------------------------- | ---------------------------------------------------------------- | -------------------------------------------------------------- |\n",
    "| **Przeznaczenie**       | Uniwersalne obliczenia, logika, zadania sekwencyjne            | Obliczenia r√≥wnoleg≈Çe (grafika, ML, symulacje)                   | Specjalizowane pod TensorFlow / AI                             |\n",
    "| **Architektura**        | Kilka‚Äìkilkana≈õcie rdzeni zoptymalizowanych do pracy szeregowej | TysiƒÖce prostszych rdzeni zoptymalizowanych do pracy r√≥wnoleg≈Çej | Dedykowane jednostki MAC (matrix multiply‚Äìaccumulate)          |\n",
    "| **Optymalizacja**       | Og√≥lne obliczenia, operacje logiczne i kontrolne               | Obliczenia macierzowe, wektory, deep learning                    | Du≈ºe operacje tensorowe, deep learning w TensorFlow/JAX        |\n",
    "| **Jƒôzyki / frameworki** | Wszystko (Python, C, Java, itd.)                               | PyTorch, TensorFlow, CUDA, OpenCL                                | TensorFlow, Keras, JAX (PyTorch ‚Äì ograniczone wsparcie)        |\n",
    "| **Szybko≈õƒá w ML**       | Najwolniejszy przy du≈ºych sieciach neuronowych                 | Du≈ºo szybszy ni≈º CPU dziƒôki r√≥wnoleg≈Ço≈õci                        | Czƒôsto jeszcze szybszy ni≈º GPU (ale tylko dla TF/JAX)          |\n",
    "| **Elastyczno≈õƒá**        | Najbardziej uniwersalny                                        | Uniwersalny + silne wsparcie w ML                                | Mocno wyspecjalizowany (najlepiej dzia≈Ça w ekosystemie Google) |\n",
    "| **Dostƒôpno≈õƒá**          | Ka≈ºdy komputer/serwer                                          | PC z kartƒÖ NVIDIA, chmura (AWS, Azure, GCP, Colab)               | G≈Ç√≥wnie Google Cloud, Google Colab                             |\n",
    "| **Koszt**               | Najta≈Ñszy (bo zawsze jest)                                     | Dro≈ºszy (GPU do ML: od kilkuset do kilkudziesiƒôciu tys. z≈Ç)      | Wynajem w chmurze ‚Äì rozliczenie za czas u≈ºycia                 |\n",
    "| **Kiedy u≈ºywaƒá?**       | Ma≈Çe modele, testy, logika aplikacji                           | Trening i inferencja du≈ºych modeli w PyTorch/TF                  | Du≈ºe modele w TensorFlow/JAX na Colab/GCP                      |\n",
    "\n",
    "---\n",
    "\n",
    "üëâ W skr√≥cie:\n",
    "\n",
    "* **CPU** ‚Äì zawsze dostƒôpny, wolny w ML, dobry do prototyp√≥w i ma≈Çych modeli.\n",
    "* **GPU** ‚Äì najlepszy wyb√≥r dla PyTorch i wiƒôkszo≈õci framework√≥w ML.\n",
    "* **TPU** ‚Äì super wydajny, ale tylko w TensorFlow/JAX i g≈Ç√≥wnie w Google Cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU EXAMPLE - COLAB\n",
    "\n",
    "## Kroki w Colab\n",
    "\n",
    "1. W≈ÇƒÖcz **TPU** w Colab:\n",
    "   `Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí TPU`\n",
    "\n",
    "2. Uruchom poni≈ºszy kod w kom√≥rkach:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# --- konfiguracja TPU ---\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # znajd≈∫ TPU\n",
    "    print(\"UrzƒÖdzenie TPU:\", tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"TPU niedostƒôpne, u≈ºywam CPU/GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ≈Åadowanie danych MNIST\n",
    "\n",
    "```python\n",
    "# Dane: rƒôcznie pisane cyfry 28x28\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalizacja\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Dodanie wymiaru kana≈Çu (28,28) -> (28,28,1)\n",
    "x_train = x_train[..., None]\n",
    "x_test = x_test[..., None]\n",
    "\n",
    "print(\"Shape train:\", x_train.shape, \"Shape test:\", x_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Definicja modelu na TPU\n",
    "\n",
    "> dziƒôki `strategy.scope()` model i optymalizator dzia≈ÇajƒÖ na TPU\n",
    "\n",
    "```python\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(28,28,1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Trenowanie modelu\n",
    "\n",
    "```python\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Ewaluacja\n",
    "\n",
    "```python\n",
    "model.evaluate(x_test, y_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚ú® Ten kod powinien uruchomiƒá trening CNN na **TPU** w Colab. Bƒôdzie zauwa≈ºalnie szybszy ni≈º na CPU, a czasami szybszy ni≈º GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
